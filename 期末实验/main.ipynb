{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "|课程名称：机器学习|学生姓名：邓力予|学生学号：20201910442|\n",
    "|-|-|-|\n",
    "|指导老师：胡三丰|实验名称：机器学习期末实验|\n",
    "|学院：数学与统计学院|专业：数据科学与大数据技术|年级：2020级|\n",
    "\n",
    "# 模型描述\n",
    "\n",
    "使用集成方法 VotingClassifier 的硬投票模式，我们可以对 Logistic 回归、SVM、决策树、神经网络等多种模型预测的结果进行投票，少数服从多数最终决定预测结果。\n",
    "\n",
    "## Logistic 回归\n",
    "\n",
    "Logistic回归模型（Logistic regression）是一种统计模型，常用于分类问题。它是广义线性模型的一种，通过使用logistic函数（也称为Sigmoid函数）将线性回归的结果转化为概率值，使其适用于二分类问题。不同于线性回归中对于参数的推导，我们在这里运用的方式不再是最小二乘法，而是极大似然估计。\n",
    "\n",
    "多重线性回归模型要求因变量是连续型的正态分布变量，且自变量与因变量呈线性关系。当因变量是分类变量，且自变量与因变量不呈线性关系时，就不能确足多重线性回归模型的适用条件。此时，处理该类资料常用Logistic回归模型。Logistic回归分析属于非线性回归，它是研究因变量为二项分类或多项分类结果与某些影响因素之间关系的一种多重回归分析方法。\n",
    "\n",
    "Logistic回归模型假设输入变量X与输出变量Y之间存在一种非线性的关系。模型的基本形式可以表示为：\n",
    "\n",
    "$$P(Y=1|X) = sigmoid(β_0 + β_1X_1 + β_2X_2 + … + β_nX_n)$$\n",
    "\n",
    "其中，P(Y=1|X)表示在给定输入变量X的条件下，输出变量Y为1的概率。sigmoid函数是一种常见的激活函数，用于将线性函数的结果映射到0到1之间。它的数学形式为：\n",
    "\n",
    "$$sigmoid(z)=\\frac{1} {(1 + e^{(-z)} )}$$\n",
    "\n",
    "其中，z表示线性函数的结果。\n",
    "\n",
    "在Logistic回归中，模型的关键部分是参数$β_0、β_1、β_2、…、β_n$，它们用于描述输入变量与输出变量之间的关系。这些参数通过最大似然估计或梯度下降等优化算法来估计。优化的目标是最大化给定观测数据集的似然函数。\n",
    "\n",
    "为了进行模型训练，我们需要一个带有已知输出变量Y的训练数据集。在训练阶段，我们通过最大化似然函数来找到最优的参数值，使得模型的预测结果与观测值之间的差距最小化。一旦模型被训练好，就可以用它来对新的输入数据进行分类预测。\n",
    "\n",
    "\n",
    "除了二分类问题，Logistic回归模型也可以扩展到多分类问题。一种常见的方法是使用一对多（One-vs-Rest）策略，将多分类问题拆分为多个二分类问题，然后分别训练多个Logistic回归模型。每个模型都用于区分一个类别与其他所有类别的组合。\n",
    "\n",
    "\n",
    "Logistic回归模型具有以下特点和优势：\n",
    "\n",
    "\n",
    "1. 简单而高效：Logistic回归是一种简单而高效的分类算法，计算成本低，易于实现。\n",
    "\n",
    "\n",
    "2. 可解释性强：模型的结果可以解释为某个特征对于分类的影响程度，通过参数的正负可以判断特征对于分类的贡献正负。\n",
    "\n",
    "\n",
    "3. 线性可分性无要求：Logistic回归模型对于输入变量的线性可分性没有要求，可以处理非线性关系。\n",
    "\n",
    "\n",
    "4. 概率输出：Logistic回归模型输出的是概率值，可以直接用于判断分类的概率。\n",
    "\n",
    "\n",
    "然而，Logistic回归模型也有一些限制和局限性：\n",
    "\n",
    "\n",
    "1. 特征相关性问题：当输入特征之间存在高度相关性时，Logistic回归可能出现多重共线性问题，影响模型的稳定性和解释性。\n",
    "\n",
    "\n",
    "2. 非线性关系的限制：尽管Logistic回归可以处理非线性关系，但对于非常复杂的非线性问题，模型的拟合能力有限。\n",
    "\n",
    "\n",
    "3. 弱分类边界：Logistic回归模型假设了特征之间的线性关系，对于具有复杂边界的数据集可能无法很好地拟合。\n",
    "\n",
    "\n",
    "总之，Logistic回归模型是一种简单而有效的分类算法，适用于二分类和多分类问题。它通过将线性回归结果转化为概率值，实现了对于分类问题的建模和预测。然而，在应用中需要注意数据的特性和模型的局限性，以确保模型的稳定性和预测能力。\n",
    "\n",
    "## 朴素贝叶斯简述\n",
    "\n",
    "朴素贝叶斯方法是一组基于应用贝叶斯定理的监督学习算法，在给定类变量值的情况下，每对特征之间都有条件独立的“朴素”假设。贝叶斯定理陈述了以下关系，给定类变量$y$和通过$x_1$，$x_2$的相关特征向量：\n",
    "\n",
    "$$P(y|x_1,\\ldots,x_n)=\\frac{P(y)P(x_1,\\ldots,x_n|y)}{P(x_1,\\ldots,x_n)}$$\n",
    "\n",
    "使用朴素的条件独立假设：\n",
    "\n",
    "$$P(x_i|y,x_1,\\ldots,x_{i-1},x_{i+1},\\ldots,x_n)=P(x_i|y)$$\n",
    "\n",
    "对于任意的$x_i$，这个关系被简化为：\n",
    "\n",
    "$$P(y|x_1,\\ldots,x_n)=\\frac{P(y)\\prod_{i=1}^{n}P(x_i|y)}{P(x_1,\\ldots,x_n)}$$\n",
    "\n",
    "由于给定输入$P(x_1,\\ldots,x_n)$是常数，我们可以使用以下分类规则：\n",
    "\n",
    "$$P(y|x_1,\\ldots,x_n)\\propto P(y)\\prod_{i=1}^{n}P(x_i|y)$$\n",
    "\n",
    "$$\\Downarrow$$\n",
    "\n",
    "$$\\hat{y}=\\arg\\max_{y}P(y)\\prod_{i=1}^{n}P(x_i|y)$$\n",
    "\n",
    "我们可以使用最大后验概率（MAP）来估计$P(y)$和$P(x_i|y)$；前者是训练集中$y$类的相对频率。\n",
    "\n",
    "不同的朴素贝叶斯分类器的区别主要在于它们对$P(x_i|y)$的分布所做的假设。\n",
    "\n",
    "显然，尽管朴素的贝叶斯分类器过于简化了假设，但它在许多实际问题中都表现得很好，比如应用最广的文档分类和垃圾邮件过滤。它们需要少量的训练数据来估计必要的参数。\n",
    "\n",
    "与更复杂的方法相比，朴素的贝叶斯学习者和分类器可以非常快速。类条件特征分布的解耦意味着每个分布可以独立地估计为一维分布。这反过来有助于缓解高维数据带来的问题。\n",
    "\n",
    "## SVM\n",
    "\n",
    "支持向量机（support vector machines, SVM）是一种二分类模型，它的基本模型是定义在特征空间上的间隔最大的线性分类器，间隔最大使它有别于感知机；SVM还包括核技巧，这使它成为实质上的非线性分类器。SVM的的学习策略就是间隔最大化，可形式化为一个求解凸二次规划的问题，也等价于正则化的合页损失函数的最小化问题。SVM的的学习算法就是求解凸二次规划的最优化算法。\n",
    "\n",
    "## 随机森林\n",
    "随机森林就是通过集成学习的Bagging思想将多棵树集成的一种算法：它的基本单元就是决策树。每棵树的按照如下规则生成：\n",
    "\n",
    "1. 如果训练集大小为N，对于每棵树而言，随机且有放回地从训练集中的抽取N个训练样本（就是bootstrap sample方法, 拔靴法采样）作为该树的训练集；从这里我们可以知道：每棵树的训练集都是不同的，而且里面包含重复的训练样本。\n",
    "\n",
    "2. 如果存在M个特征，则在每个节点分裂的时候，从M中随机选择m个特征维度（m << M），使用这些m个特征维度中最佳特征(最大化信息增益)来分割节点。在森林生长期间，m的值保持不变。\n",
    "\n",
    "一开始我们提到的随机森林中的“随机”就是指的这里的两个随机性。两个随机性的引入对随机森林的分类性能至关重要。由于它们的引入，使得随机森林不容易陷入过拟合，并且具有很好得抗噪能力（比如：对缺省值不敏感）。\n",
    "\n",
    "随机森林分类效果（错误率）与两个因素有关：\n",
    "\n",
    "1. 森林中任意两棵树的相关性：相关性越大，错误率越大；（弱分类器应该good且different）\n",
    "\n",
    "2. 森林中每棵树的分类能力：每棵树的分类能力越强，整个森林的错误率越低。（弱分类器应该good且different）\n",
    "\n",
    "减小特征选择个数m，树的相关性和分类能力也会相应的降低；增大m，两者也会随之增大。所以关键问题是如何选择最优的m，这也是随机森林的一个重要参数。\n",
    "\n",
    "## 决策树\n",
    "\n",
    "Decision Trees (DTs) are a non-parametric supervised learning method used for classification and regression. The goal is to create a model that predicts the value of a target variable by learning simple decision rules inferred from the data features. A tree can be seen as a piecewise constant approximation.\n",
    "\n",
    "For instance, in the example below, decision trees learn from data to approximate a sine curve with a set of if-then-else decision rules. The deeper the tree, the more complex the decision rules and the fitter the model.\n",
    "\n",
    "Some advantages of decision trees are:\n",
    "\n",
    "- Simple to understand and to interpret. Trees can be visualized.\n",
    "\n",
    "- Requires little data preparation. Other techniques often require data normalization, dummy variables need to be created and blank values to be removed. Note however that this module does not support missing values.\n",
    "\n",
    "- The cost of using the tree (i.e., predicting data) is logarithmic in the number of data points used to train the tree.\n",
    "\n",
    "- Able to handle both numerical and categorical data. However, the scikit-learn implementation does not support categorical variables for now. Other techniques are usually specialized in analyzing datasets that have only one type of variable. See algorithms for more information.\n",
    "\n",
    "- Able to handle multi-output problems.\n",
    "\n",
    "- Uses a white box model. If a given situation is observable in a model, the explanation for the condition is easily explained by boolean logic. By contrast, in a black box model (e.g., in an artificial neural network), results may be more difficult to interpret.\n",
    "\n",
    "- Possible to validate a model using statistical tests. That makes it possible to account for the reliability of the model.\n",
    "\n",
    "- Performs well even if its assumptions are somewhat violated by the true model from which the data were generated.\n",
    "\n",
    "The disadvantages of decision trees include:\n",
    "\n",
    "- Decision-tree learners can create over-complex trees that do not generalize the data well. This is called overfitting. Mechanisms such as pruning, setting the minimum number of samples required at a leaf node or setting the maximum depth of the tree are necessary to avoid this problem.\n",
    "\n",
    "- Decision trees can be unstable because small variations in the data might result in a completely different tree being generated. This problem is mitigated by using decision trees within an ensemble.\n",
    "\n",
    "- Predictions of decision trees are neither smooth nor continuous, but piecewise constant approximations as seen in the above figure. Therefore, they are not good at extrapolation.\n",
    "\n",
    "- The problem of learning an optimal decision tree is known to be NP-complete under several aspects of optimality and even for simple concepts. Consequently, practical decision-tree learning algorithms are based on heuristic algorithms such as the greedy algorithm where locally optimal decisions are made at each node. Such algorithms cannot guarantee to return the globally optimal decision tree. This can be mitigated by training multiple trees in an ensemble learner, where the features and samples are randomly sampled with replacement.\n",
    "\n",
    "- There are concepts that are hard to learn because decision trees do not express them easily, such as XOR, parity or multiplexer problems.\n",
    "\n",
    "- Decision tree learners create biased trees if some classes dominate. It is therefore recommended to balance the dataset prior to fitting with the decision tree.\n",
    "\n",
    "## MLP\n",
    "\n",
    "MLP又名多层感知机，也叫人工神经网络（ANN，Artificial Neural Network），除了输入输出层，它中间可以有多个隐藏层，如果没有隐藏层即可解决线性可划分的数据问题。最简单的MLP模型只包含一个隐藏层，即三层的结构。\n",
    "\n",
    "多层感知机的层与层之间是全连接的（全连接的意思就是：上一层的任何一个神经元与下一层的所有神经元都有连接）。多层感知机最底层是输入层，中间是隐藏层，最后是输出层。\n",
    "\n",
    "# 设计糖尿病预测算法\n",
    "\n",
    "## 导入必要包"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pandas import read_csv\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import VotingClassifier, RandomForestClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.naive_bayes import GaussianNB"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 数据的读取和预处理\n",
    "\n",
    "- 读取数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_meta = read_csv(\"diabetes_dataset.csv.xz\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 数据类型转换"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_meta_replaced=data_meta.replace({\"gender\":{\"Female\":0, \"Male\":1, \"Other\":2}, \"hypertension\":{0:False, 1:True}, \"heart_disease\":{1:True, 0:False}, \"smoking_history\":{\"never\":0, \"former\":1, \"current\":2, \"not current\":3, \"ever\":4, \"No Info\":5}, \"diabetes\":{0:False, 1:True}}, inplace=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 拆分测试集和训练集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train, data_test = train_test_split(data_meta_replaced, train_size=0.75)\n",
    "\n",
    "data_train_x = data_train.iloc[:, 1:]\n",
    "data_train_y = data_train[\"diabetes\"]\n",
    "\n",
    "data_test_x = data_test.iloc[:, 1:]\n",
    "data_test_y = data_test[\"diabetes\"]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "由于数据是不平衡的，需要计算正反例比例，后续训练模型时使用"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_class_weight={True:len(data_train[data_train[\"diabetes\"]==True]), False:len(data_train[data_train[\"diabetes\"]==False])}"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 模型的建立\n",
    "\n",
    "采用硬投票的集成学习算法，其中包含六种基础算法进程投票：\n",
    "\n",
    "- Logistic 回归\n",
    "\n",
    "- SVM\n",
    "\n",
    "- 决策树\n",
    "\n",
    "- 随机森林\n",
    "\n",
    "- MLP 神经网络\n",
    "\n",
    "- 朴素贝叶斯"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "a:\\Program Files\\PF\\Miniconda3\\envs\\machine-learning\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-4 {color: black;background-color: white;}#sk-container-id-4 pre{padding: 0;}#sk-container-id-4 div.sk-toggleable {background-color: white;}#sk-container-id-4 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-4 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-4 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-4 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-4 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-4 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-4 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-4 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-4 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-4 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-4 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-4 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-4 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-4 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-4 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-4 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-4 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-4 div.sk-item {position: relative;z-index: 1;}#sk-container-id-4 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-4 div.sk-item::before, #sk-container-id-4 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-4 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-4 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-4 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-4 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-4 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-4 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-4 div.sk-label-container {text-align: center;}#sk-container-id-4 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-4 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-4\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>VotingClassifier(estimators=[(&#x27;Logistic 回归&#x27;,\n",
       "                              LogisticRegression(class_weight={False: 68616,\n",
       "                                                               True: 6384})),\n",
       "                             (&#x27;SVM&#x27;,\n",
       "                              SVC(class_weight={False: 68616, True: 6384})),\n",
       "                             (&#x27;决策树&#x27;,\n",
       "                              DecisionTreeClassifier(class_weight={False: 68616,\n",
       "                                                                   True: 6384})),\n",
       "                             (&#x27;随机森林&#x27;,\n",
       "                              RandomForestClassifier(class_weight={False: 68616,\n",
       "                                                                   True: 6384})),\n",
       "                             (&#x27;MLP 神经网络&#x27;, MLPClassifier()),\n",
       "                             (&#x27;朴素贝叶斯&#x27;, GaussianNB())])</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-16\" type=\"checkbox\" ><label for=\"sk-estimator-id-16\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">VotingClassifier</label><div class=\"sk-toggleable__content\"><pre>VotingClassifier(estimators=[(&#x27;Logistic 回归&#x27;,\n",
       "                              LogisticRegression(class_weight={False: 68616,\n",
       "                                                               True: 6384})),\n",
       "                             (&#x27;SVM&#x27;,\n",
       "                              SVC(class_weight={False: 68616, True: 6384})),\n",
       "                             (&#x27;决策树&#x27;,\n",
       "                              DecisionTreeClassifier(class_weight={False: 68616,\n",
       "                                                                   True: 6384})),\n",
       "                             (&#x27;随机森林&#x27;,\n",
       "                              RandomForestClassifier(class_weight={False: 68616,\n",
       "                                                                   True: 6384})),\n",
       "                             (&#x27;MLP 神经网络&#x27;, MLPClassifier()),\n",
       "                             (&#x27;朴素贝叶斯&#x27;, GaussianNB())])</pre></div></div></div><div class=\"sk-parallel\"><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><label>Logistic 回归</label></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-17\" type=\"checkbox\" ><label for=\"sk-estimator-id-17\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LogisticRegression</label><div class=\"sk-toggleable__content\"><pre>LogisticRegression(class_weight={False: 68616, True: 6384})</pre></div></div></div></div></div></div><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><label>SVM</label></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-18\" type=\"checkbox\" ><label for=\"sk-estimator-id-18\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">SVC</label><div class=\"sk-toggleable__content\"><pre>SVC(class_weight={False: 68616, True: 6384})</pre></div></div></div></div></div></div><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><label>决策树</label></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-19\" type=\"checkbox\" ><label for=\"sk-estimator-id-19\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">DecisionTreeClassifier</label><div class=\"sk-toggleable__content\"><pre>DecisionTreeClassifier(class_weight={False: 68616, True: 6384})</pre></div></div></div></div></div></div><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><label>随机森林</label></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-20\" type=\"checkbox\" ><label for=\"sk-estimator-id-20\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">RandomForestClassifier</label><div class=\"sk-toggleable__content\"><pre>RandomForestClassifier(class_weight={False: 68616, True: 6384})</pre></div></div></div></div></div></div><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><label>MLP 神经网络</label></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-21\" type=\"checkbox\" ><label for=\"sk-estimator-id-21\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">MLPClassifier</label><div class=\"sk-toggleable__content\"><pre>MLPClassifier()</pre></div></div></div></div></div></div><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><label>朴素贝叶斯</label></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-22\" type=\"checkbox\" ><label for=\"sk-estimator-id-22\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">GaussianNB</label><div class=\"sk-toggleable__content\"><pre>GaussianNB()</pre></div></div></div></div></div></div></div></div></div></div>"
      ],
      "text/plain": [
       "VotingClassifier(estimators=[('Logistic 回归',\n",
       "                              LogisticRegression(class_weight={False: 68616,\n",
       "                                                               True: 6384})),\n",
       "                             ('SVM',\n",
       "                              SVC(class_weight={False: 68616, True: 6384})),\n",
       "                             ('决策树',\n",
       "                              DecisionTreeClassifier(class_weight={False: 68616,\n",
       "                                                                   True: 6384})),\n",
       "                             ('随机森林',\n",
       "                              RandomForestClassifier(class_weight={False: 68616,\n",
       "                                                                   True: 6384})),\n",
       "                             ('MLP 神经网络', MLPClassifier()),\n",
       "                             ('朴素贝叶斯', GaussianNB())])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = VotingClassifier(estimators=[\n",
    "    ('Logistic 回归', LogisticRegression(class_weight=data_class_weight)),\n",
    "    ('SVM', SVC(class_weight=data_class_weight)),\n",
    "    ('决策树', DecisionTreeClassifier(class_weight=data_class_weight)),\n",
    "    (\"随机森林\", RandomForestClassifier(class_weight=data_class_weight)),\n",
    "    (\"MLP 神经网络\", MLPClassifier()),\n",
    "    (\"朴素贝叶斯\", GaussianNB()),\n",
    "], voting='hard')\n",
    "model.fit(data_train_x, data_train_y)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 模型的验证与总结"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n"
     ]
    }
   ],
   "source": [
    "data_pre = model.predict(data_test_x)\n",
    "print(accuracy_score(data_test_y, data_pre))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "可以看到模型在测试集上的预测正确率为100%，说明这其中可能存在过拟合现象。也同时说明了硬投票算法可以集成多种算法的优势，从而得到正确率非常高的模型。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "machine-learning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
