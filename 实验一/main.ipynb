{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "|课程名称：机器学习|学生姓名：邓力予|学生学号：20201910442|\n",
    "|-|-|-|\n",
    "|实验名称：机器学习实验一|\n",
    "|学院：数学与统计学院|专业：数据科学与大数据技术|年级：2020级|"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# 使用scikit-learn\n",
    "\n",
    "## 导入包"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import tree\n",
    "import pandas as pd\n",
    "from sklearn.metrics import accuracy_score\n",
    "import numpy as np"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 读取数据\n",
    "### 使用pandas读取数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train = pd.read_csv(\"../fashion-mnist_train.csv\")\n",
    "data_test = pd.read_csv(\"../fashion-mnist_test.csv\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 拆分数据集\n",
    "\n",
    "- 训练集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train_x = data_train.iloc[:, 1:]\n",
    "data_train_y = data_train[\"label\"]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 测试集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_test_x = data_test.iloc[:, 1:]\n",
    "data_test_y = data_test[\"label\"]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 使用scikit-learn训练模型\n",
    "\n",
    "### 训练\n",
    "\n",
    "- 实例化模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_sklearn = tree.DecisionTreeClassifier()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 训练模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>DecisionTreeClassifier()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">DecisionTreeClassifier</label><div class=\"sk-toggleable__content\"><pre>DecisionTreeClassifier()</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "DecisionTreeClassifier()"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_sklearn.fit(data_train_x, y=data_train_y)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 测试"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_test_y_pre = model_sklearn.predict(data_test_x)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 计算正确率"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7973\n"
     ]
    }
   ],
   "source": [
    "score_sklearn = accuracy_score(data_test_y, data_test_y_pre)\n",
    "print(score_sklearn)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 不使用scikit-learn\n",
    "\n",
    "## 构建决策树\n",
    "\n",
    "- 计算信息熵"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cal_information_entropy(data):\n",
    "    data_label = data.iloc[:,-1]\n",
    "    label_class =data_label.value_counts() #总共有多少类\n",
    "    Ent = 0\n",
    "    for k in label_class.keys():\n",
    "        p_k = label_class[k]/len(data_label)\n",
    "        Ent += -p_k*np.log2(p_k)\n",
    "    return Ent"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 计算给定数据属性a的信息增益"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cal_information_gain(data, a):\n",
    "    Ent = cal_information_entropy(data)\n",
    "    feature_class = data[a].value_counts() #特征有多少种可能\n",
    "    gain = 0\n",
    "    for v in feature_class.keys():\n",
    "        weight = feature_class[v]/data.shape[0]\n",
    "        Ent_v = cal_information_entropy(data.loc[data[a] == v])\n",
    "        gain += weight*Ent_v\n",
    "    return Ent - gain"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 先计算固有值intrinsic_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cal_gain_ratio(data , a):\n",
    "    IV_a = 0\n",
    "    feature_class = data[a].value_counts()  # 特征有多少种可能\n",
    "    for v in feature_class.keys():\n",
    "        weight = feature_class[v]/data.shape[0]\n",
    "        IV_a += -weight*np.log2(weight)\n",
    "    gain_ration = cal_information_gain(data,a)/IV_a\n",
    "    return gain_ration"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 获取标签最多的那一类"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_most_label(data):\n",
    "    data_label = data.iloc[:,-1]\n",
    "    label_sort = data_label.value_counts(sort=True)\n",
    "    return label_sort.keys()[0]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 挑选最优特征，即在信息增益大于平均水平的特征中选取增益率最高的特征"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_best_feature(data):\n",
    "    features = data.columns[:-1]\n",
    "    res = {}\n",
    "    for a in features:\n",
    "        temp = cal_information_gain(data, a)\n",
    "        gain_ration = cal_gain_ratio(data,a)\n",
    "        res[a] = (temp,gain_ration)\n",
    "    res = sorted(res.items(),key=lambda x:x[1][0],reverse=True) #按信息增益排名\n",
    "    res_avg = sum([x[1][0] for x in res])/len(res) #信息增益平均水平\n",
    "    good_res = [x for x in res if x[1][0] >= res_avg] #选取信息增益高于平均水平的特征\n",
    "    result =sorted(good_res,key=lambda x:x[1][1],reverse=True) #将信息增益高的特征按照增益率进行排名\n",
    "    return result[0][0] #返回高信息增益中增益率最大的特征"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 将数据转化为（属性值：数据）的元组形式返回，并删除之前的特征列"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def drop_exist_feature(data, best_feature):\n",
    "    attr = pd.unique(data[best_feature])\n",
    "    new_data = [(nd, data[data[best_feature] == nd]) for nd in attr]\n",
    "    new_data = [(n[0], n[1].drop([best_feature], axis=1)) for n in new_data]\n",
    "    return new_data"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 创建决策树"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_tree(data):\n",
    "    global column_count\n",
    "    data_label = data.iloc[:,-1]\n",
    "    if len(data_label.value_counts()) == 1: #只有一类\n",
    "        return data_label.values[0]\n",
    "    if all(len(data[i].value_counts()) == 1 for i in data.iloc[:,:-1].columns): #所有数据的特征值一样，选样本最多的类作为分类结果\n",
    "        return get_most_label(data)\n",
    "    best_feature = get_best_feature(data) #根据信息增益得到的最优划分特征\n",
    "    Tree = {best_feature:{}} #用字典形式存储决策树\n",
    "    exist_vals = pd.unique(data[best_feature])  # 当前数据下最佳特征的取值\n",
    "    if len(exist_vals) != len(column_count[best_feature]):  # 如果特征的取值相比于原来的少了\n",
    "        no_exist_attr = set(column_count[best_feature]) - set(exist_vals)  # 少的那些特征\n",
    "        for no_feat in no_exist_attr:\n",
    "            Tree[best_feature][no_feat] = get_most_label(data)  # 缺失的特征分类为当前类别最多的\n",
    "    for item in drop_exist_feature(data,best_feature): #根据特征值的不同递归创建决策树\n",
    "        Tree[best_feature][item[0]] = create_tree(item[1])\n",
    "    return Tree"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 预测"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(Tree , test_data):\n",
    "    first_feature = list(Tree.keys())[0]\n",
    "    second_dict = Tree[first_feature]\n",
    "    input_first = test_data.get(first_feature)\n",
    "    input_value = second_dict[input_first]\n",
    "    if isinstance(input_value , dict): #判断分支还是不是字典\n",
    "        class_label = predict(input_value, test_data)\n",
    "    else:\n",
    "        class_label = input_value\n",
    "    return class_label"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 训练\n",
    "\n",
    "- 统计每个特征的取值情况作为全局变量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "column_count = dict([(ds, list(pd.unique(data_train[ds]))) for ds in data_train.iloc[:, :-1].columns])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 训练决策树"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[23], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m----> 2\u001b[0m     \u001b[39mwith\u001b[39;00m \u001b[39mopen\u001b[39;49m(\u001b[39m\"\u001b[39;49m\u001b[39mTiree.txt\u001b[39;49m\u001b[39m\"\u001b[39;49m,\u001b[39m\"\u001b[39;49m\u001b[39mr\u001b[39;49m\u001b[39m\"\u001b[39;49m) \u001b[39mas\u001b[39;00m file_tree:\n\u001b[0;32m      3\u001b[0m         dicision_Tree\u001b[39m=\u001b[39m\u001b[39meval\u001b[39m(file_tree\u001b[39m.\u001b[39mread())\n",
      "File \u001b[1;32ma:\\Program Files\\PF\\Miniconda3\\envs\\machine-learning\\Lib\\site-packages\\IPython\\core\\interactiveshell.py:284\u001b[0m, in \u001b[0;36m_modified_open\u001b[1;34m(file, *args, **kwargs)\u001b[0m\n\u001b[0;32m    278\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m    279\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mIPython won\u001b[39m\u001b[39m'\u001b[39m\u001b[39mt let you open fd=\u001b[39m\u001b[39m{\u001b[39;00mfile\u001b[39m}\u001b[39;00m\u001b[39m by default \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    280\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mas it is likely to crash IPython. If you know what you are doing, \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    281\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39myou can use builtins\u001b[39m\u001b[39m'\u001b[39m\u001b[39m open.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    282\u001b[0m     )\n\u001b[1;32m--> 284\u001b[0m \u001b[39mreturn\u001b[39;00m io_open(file, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'Tiree.txt'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[23], line 5\u001b[0m\n\u001b[0;32m      3\u001b[0m         dicision_Tree\u001b[39m=\u001b[39m\u001b[39meval\u001b[39m(file_tree\u001b[39m.\u001b[39mread())\n\u001b[0;32m      4\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mFileNotFoundError\u001b[39;00m:\n\u001b[1;32m----> 5\u001b[0m     dicision_Tree \u001b[39m=\u001b[39m create_tree(data_train)\n\u001b[0;32m      6\u001b[0m     \u001b[39mwith\u001b[39;00m \u001b[39mopen\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mTree.txt\u001b[39m\u001b[39m\"\u001b[39m,\u001b[39m\"\u001b[39m\u001b[39mw\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39mas\u001b[39;00m file_tree:\n\u001b[0;32m      7\u001b[0m         file_tree\u001b[39m.\u001b[39mwrite(\u001b[39mstr\u001b[39m(dicision_Tree))\n",
      "Cell \u001b[1;32mIn[17], line 7\u001b[0m, in \u001b[0;36mcreate_tree\u001b[1;34m(data)\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mall\u001b[39m(\u001b[39mlen\u001b[39m(data[i]\u001b[39m.\u001b[39mvalue_counts()) \u001b[39m==\u001b[39m \u001b[39m1\u001b[39m \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m data\u001b[39m.\u001b[39miloc[:,:\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m]\u001b[39m.\u001b[39mcolumns): \u001b[39m#所有数据的特征值一样，选样本最多的类作为分类结果\u001b[39;00m\n\u001b[0;32m      6\u001b[0m     \u001b[39mreturn\u001b[39;00m get_most_label(data)\n\u001b[1;32m----> 7\u001b[0m best_feature \u001b[39m=\u001b[39m get_best_feature(data) \u001b[39m#根据信息增益得到的最优划分特征\u001b[39;00m\n\u001b[0;32m      8\u001b[0m Tree \u001b[39m=\u001b[39m {best_feature:{}} \u001b[39m#用字典形式存储决策树\u001b[39;00m\n\u001b[0;32m      9\u001b[0m exist_vals \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39munique(data[best_feature])  \u001b[39m# 当前数据下最佳特征的取值\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[15], line 5\u001b[0m, in \u001b[0;36mget_best_feature\u001b[1;34m(data)\u001b[0m\n\u001b[0;32m      3\u001b[0m res \u001b[39m=\u001b[39m {}\n\u001b[0;32m      4\u001b[0m \u001b[39mfor\u001b[39;00m a \u001b[39min\u001b[39;00m features:\n\u001b[1;32m----> 5\u001b[0m     temp \u001b[39m=\u001b[39m cal_information_gain(data, a)\n\u001b[0;32m      6\u001b[0m     gain_ration \u001b[39m=\u001b[39m cal_gain_ratio(data,a)\n\u001b[0;32m      7\u001b[0m     res[a] \u001b[39m=\u001b[39m (temp,gain_ration)\n",
      "Cell \u001b[1;32mIn[12], line 7\u001b[0m, in \u001b[0;36mcal_information_gain\u001b[1;34m(data, a)\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[39mfor\u001b[39;00m v \u001b[39min\u001b[39;00m feature_class\u001b[39m.\u001b[39mkeys():\n\u001b[0;32m      6\u001b[0m     weight \u001b[39m=\u001b[39m feature_class[v]\u001b[39m/\u001b[39mdata\u001b[39m.\u001b[39mshape[\u001b[39m0\u001b[39m]\n\u001b[1;32m----> 7\u001b[0m     Ent_v \u001b[39m=\u001b[39m cal_information_entropy(data\u001b[39m.\u001b[39;49mloc[data[a] \u001b[39m==\u001b[39;49m v])\n\u001b[0;32m      8\u001b[0m     gain \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m weight\u001b[39m*\u001b[39mEnt_v\n\u001b[0;32m      9\u001b[0m \u001b[39mreturn\u001b[39;00m Ent \u001b[39m-\u001b[39m gain\n",
      "Cell \u001b[1;32mIn[11], line 3\u001b[0m, in \u001b[0;36mcal_information_entropy\u001b[1;34m(data)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mcal_information_entropy\u001b[39m(data):\n\u001b[0;32m      2\u001b[0m     data_label \u001b[39m=\u001b[39m data\u001b[39m.\u001b[39miloc[:,\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m]\n\u001b[1;32m----> 3\u001b[0m     label_class \u001b[39m=\u001b[39mdata_label\u001b[39m.\u001b[39;49mvalue_counts() \u001b[39m#总共有多少类\u001b[39;00m\n\u001b[0;32m      4\u001b[0m     Ent \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n\u001b[0;32m      5\u001b[0m     \u001b[39mfor\u001b[39;00m k \u001b[39min\u001b[39;00m label_class\u001b[39m.\u001b[39mkeys():\n",
      "File \u001b[1;32ma:\\Program Files\\PF\\Miniconda3\\envs\\machine-learning\\Lib\\site-packages\\pandas\\core\\base.py:980\u001b[0m, in \u001b[0;36mIndexOpsMixin.value_counts\u001b[1;34m(self, normalize, sort, ascending, bins, dropna)\u001b[0m\n\u001b[0;32m    894\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mvalue_counts\u001b[39m(\n\u001b[0;32m    895\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[0;32m    896\u001b[0m     normalize: \u001b[39mbool\u001b[39m \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    900\u001b[0m     dropna: \u001b[39mbool\u001b[39m \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m,\n\u001b[0;32m    901\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Series:\n\u001b[0;32m    902\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    903\u001b[0m \u001b[39m    Return a Series containing counts of unique values.\u001b[39;00m\n\u001b[0;32m    904\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    978\u001b[0m \u001b[39m    dtype: int64\u001b[39;00m\n\u001b[0;32m    979\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 980\u001b[0m     \u001b[39mreturn\u001b[39;00m value_counts(\n\u001b[0;32m    981\u001b[0m         \u001b[39mself\u001b[39;49m,\n\u001b[0;32m    982\u001b[0m         sort\u001b[39m=\u001b[39;49msort,\n\u001b[0;32m    983\u001b[0m         ascending\u001b[39m=\u001b[39;49mascending,\n\u001b[0;32m    984\u001b[0m         normalize\u001b[39m=\u001b[39;49mnormalize,\n\u001b[0;32m    985\u001b[0m         bins\u001b[39m=\u001b[39;49mbins,\n\u001b[0;32m    986\u001b[0m         dropna\u001b[39m=\u001b[39;49mdropna,\n\u001b[0;32m    987\u001b[0m     )\n",
      "File \u001b[1;32ma:\\Program Files\\PF\\Miniconda3\\envs\\machine-learning\\Lib\\site-packages\\pandas\\core\\algorithms.py:1001\u001b[0m, in \u001b[0;36mvalue_counts\u001b[1;34m(values, sort, ascending, normalize, bins, dropna)\u001b[0m\n\u001b[0;32m    998\u001b[0m         result \u001b[39m=\u001b[39m Series(counts, index\u001b[39m=\u001b[39midx, name\u001b[39m=\u001b[39mname)\n\u001b[0;32m   1000\u001b[0m \u001b[39mif\u001b[39;00m sort:\n\u001b[1;32m-> 1001\u001b[0m     result \u001b[39m=\u001b[39m result\u001b[39m.\u001b[39;49msort_values(ascending\u001b[39m=\u001b[39;49mascending)\n\u001b[0;32m   1003\u001b[0m \u001b[39mif\u001b[39;00m normalize:\n\u001b[0;32m   1004\u001b[0m     result \u001b[39m=\u001b[39m result \u001b[39m/\u001b[39m counts\u001b[39m.\u001b[39msum()\n",
      "File \u001b[1;32ma:\\Program Files\\PF\\Miniconda3\\envs\\machine-learning\\Lib\\site-packages\\pandas\\util\\_decorators.py:331\u001b[0m, in \u001b[0;36mdeprecate_nonkeyword_arguments.<locals>.decorate.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    325\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(args) \u001b[39m>\u001b[39m num_allow_args:\n\u001b[0;32m    326\u001b[0m     warnings\u001b[39m.\u001b[39mwarn(\n\u001b[0;32m    327\u001b[0m         msg\u001b[39m.\u001b[39mformat(arguments\u001b[39m=\u001b[39m_format_argument_list(allow_args)),\n\u001b[0;32m    328\u001b[0m         \u001b[39mFutureWarning\u001b[39;00m,\n\u001b[0;32m    329\u001b[0m         stacklevel\u001b[39m=\u001b[39mfind_stack_level(),\n\u001b[0;32m    330\u001b[0m     )\n\u001b[1;32m--> 331\u001b[0m \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[1;32ma:\\Program Files\\PF\\Miniconda3\\envs\\machine-learning\\Lib\\site-packages\\pandas\\core\\series.py:3768\u001b[0m, in \u001b[0;36mSeries.sort_values\u001b[1;34m(self, axis, ascending, inplace, kind, na_position, ignore_index, key)\u001b[0m\n\u001b[0;32m   3766\u001b[0m \u001b[39m# GH 35922. Make sorting stable by leveraging nargsort\u001b[39;00m\n\u001b[0;32m   3767\u001b[0m values_to_sort \u001b[39m=\u001b[39m ensure_key_mapped(\u001b[39mself\u001b[39m, key)\u001b[39m.\u001b[39m_values \u001b[39mif\u001b[39;00m key \u001b[39melse\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_values\n\u001b[1;32m-> 3768\u001b[0m sorted_index \u001b[39m=\u001b[39m nargsort(values_to_sort, kind, \u001b[39mbool\u001b[39;49m(ascending), na_position)\n\u001b[0;32m   3770\u001b[0m result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_constructor(\n\u001b[0;32m   3771\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_values[sorted_index], index\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mindex[sorted_index]\n\u001b[0;32m   3772\u001b[0m )\n\u001b[0;32m   3774\u001b[0m \u001b[39mif\u001b[39;00m ignore_index:\n",
      "File \u001b[1;32ma:\\Program Files\\PF\\Miniconda3\\envs\\machine-learning\\Lib\\site-packages\\pandas\\core\\sorting.py:444\u001b[0m, in \u001b[0;36mnargsort\u001b[1;34m(items, kind, ascending, na_position, key, mask)\u001b[0m\n\u001b[0;32m    441\u001b[0m \u001b[39m# Finally, place the NaNs at the end or the beginning according to\u001b[39;00m\n\u001b[0;32m    442\u001b[0m \u001b[39m# na_position\u001b[39;00m\n\u001b[0;32m    443\u001b[0m \u001b[39mif\u001b[39;00m na_position \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mlast\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[1;32m--> 444\u001b[0m     indexer \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39;49mconcatenate([indexer, nan_idx])\n\u001b[0;32m    445\u001b[0m \u001b[39melif\u001b[39;00m na_position \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mfirst\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[0;32m    446\u001b[0m     indexer \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mconcatenate([nan_idx, indexer])\n",
      "File \u001b[1;32m<__array_function__ internals>:200\u001b[0m, in \u001b[0;36mconcatenate\u001b[1;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "try:\n",
    "    with open(\"Tree.txt\",\"r\") as file_tree:\n",
    "        dicision_Tree=eval(file_tree.read())\n",
    "except FileNotFoundError:\n",
    "    dicision_Tree = create_tree(data_train)\n",
    "    with open(\"Tree.txt\",\"w\") as file_tree:\n",
    "        file_tree.write(str(dicision_Tree))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 测试\n",
    "\n",
    "- 使用测试集预测"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "pre_y_mytree=[]\n",
    "for i in range(10000):\n",
    "    try:\n",
    "        pre_y_mytree.append(predict(dicision_Tree, data_test_x.iloc[i, :]))\n",
    "    except KeyError:\n",
    "        pre_y_mytree.append(np.nan)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 准确率"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.1\n"
     ]
    }
   ],
   "source": [
    "right_count=0\n",
    "for i in range(10000):\n",
    "    if pre_y_mytree[i]==data_test_y[i]:\n",
    "        right_count=right_count+1\n",
    "print(right_count/10000)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "machine-learning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "toc-autonumbering": true,
  "toc-showcode": false,
  "toc-showmarkdowntxt": false
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
